<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Michael Jay Lissner</title><link href="http://michaeljaylissner.com/" rel="alternate"></link><link href="http://michaeljaylissner.com/feeds/tag/bulk-data" rel="self"></link><id>http://michaeljaylissner.com/</id><updated>2014-11-06T00:00:00-08:00</updated><entry><title>Updating Bulk Data in CourtListener…Again</title><link href="http://michaeljaylissner.com/posts/2014/11/06/updating-bulk-data-in-courtlistener-more/" rel="alternate"></link><updated>2014-11-06T00:00:00-08:00</updated><author><name>Mike Lissner</name></author><id>tag:michaeljaylissner.com,2014-11-06:posts/2014/11/06/updating-bulk-data-in-courtlistener-more/</id><summary type="html">&lt;p&gt;I &lt;a href="http://michaeljaylissner.com/posts/2014/09/28/updating-bulk-data-in-courtlistener/"&gt;wrote a few weeks ago&lt;/a&gt; about our new system for creating &lt;a href="https://www.courtlistener.com/api/bulk-info/"&gt;bulk files in CourtListener&lt;/a&gt;. The system was pretty good. The goal was and is to efficiently create one bulk file for every jurisdiction&amp;#8212;object pair in the system. So, that means one bulk file for oral arguments from Supreme Court, another for opinions from the Ninth Circuit of Appeals, another for dockets from Alabama&amp;#8217;s appellate court, etc. We have about 350 jurisdictions and four different object types right now, for a total of about 1,400 bulk&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;This system needs to be&amp;nbsp;fast.&lt;/p&gt;
&lt;p&gt;The old system that I wrote about before would create 350 open file handles at a time, and then would iterate over each item in the database, adding it to the correct file as it inspected the item. This was a beautiful system because it only had to iterate over the database once, but even after &lt;a href="https://github.com/freelawproject/courtlistener/commit/a0e4326d98e9f501ec3e69955d6b5650471686e8"&gt;performance tuning&lt;/a&gt;, it still took about 24 hours. Not good&amp;nbsp;enough. &lt;/p&gt;
&lt;p&gt;I got to thinking that it was terrible to create the entire bulk archive over and over when in reality only a few items change each day. So I created a &lt;a href="https://github.com/freelawproject/courtlistener/issues/293"&gt;bug to make bulk data creation incremental&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;This post is about that&amp;nbsp;process.&lt;/p&gt;
&lt;h2 id="the-first-approach"&gt;The First&amp;nbsp;Approach&lt;/h2&gt;
&lt;p&gt;The obvious way to do this kind of thing is to grab the bulk files you already have (as gz-compressed tar files), and add the updated items to those files. Well, I wrote up code for this, tested it pretty thoroughly and considered it done. Only to realize that, like regular files, when you create a compressed tar file with a command like&amp;nbsp;this&amp;#8230;   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;tar_files&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;all&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tarfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;all.tar.gz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;w:gz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&amp;#8230;it clobbers any old files that might have the same name. So much for&amp;nbsp;that.&lt;/p&gt;
&lt;h2 id="next-approach"&gt;Next&amp;nbsp;Approach&lt;/h2&gt;
&lt;p&gt;Well, it looked like we needed append mode for compressed tar files, but alas, from &lt;a href="https://docs.python.org/3/library/tarfile.html"&gt;the documentation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that &amp;#8216;a:gz&amp;#8217;, &amp;#8216;a:bz2&amp;#8217; or &amp;#8216;a:xz&amp;#8217; is not&amp;nbsp;possible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;a:gz&amp;#8221; means a gz-compressed tar file in &lt;strong&gt;a&lt;/strong&gt;ppend mode, so much for that idea.&amp;nbsp;Next! &lt;/p&gt;
&lt;h2 id="next-approach_1"&gt;Next&amp;nbsp;Approach&lt;/h2&gt;
&lt;p&gt;Well, you can&amp;#8217;t make gz-compressed tar files in append mode, but you can create tar files in append mode as step one, then compress them as step two. I tried this next, and again, it looked like it was working&amp;#8230;until I realized that my tar files contained copy after copy after copy of each file. I was hoping that it&amp;#8217;d simply clobber files that were previously in the file, but instead it was just putting multiple files of the same name into the&amp;nbsp;tar. &lt;/p&gt;
&lt;p&gt;Perhaps I can delete from the tar file before adding items back to it? &lt;a href="http://bytes.com/topic/python/answers/40408-deleting-tarfile"&gt;Nope, that&amp;#8217;s not possible either&lt;/a&gt;. Next&amp;nbsp;idea?&lt;/p&gt;
&lt;h2 id="final-approach"&gt;Final&amp;nbsp;Approach&lt;/h2&gt;
&lt;p&gt;I was feeling pretty frustrated by now, but there was one more approach, and that was to add an intermediate step. Instead of creating the tar files directly in Python, I could save the individual &lt;code&gt;json&lt;/code&gt; files I was previously putting into the tar file to disk, then create the compressed tar files directly from those once they&amp;#8217;re all created. We proved earlier that Python has no issues about clobbering items on disk, so that&amp;#8217;ll work nicely for incremental bulk files, which will just clobber old versions of the&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;From performance analyses of the code, most of the bottleneck is in serializing &lt;span class="caps"&gt;JSON&lt;/span&gt;, so this will change it so that gets done at most once per item in the database and then most of the remaining work will be making tar files and gz-compressing&amp;nbsp;them. &lt;/p&gt;
&lt;h2 id="whew"&gt;Whew!&lt;/h2&gt;
&lt;p&gt;I was hoping that I would be able to easily update items inside a compressed tar file, or even inside an uncompressed tar file, but that doesn&amp;#8217;t seem&amp;nbsp;possible. &lt;/p&gt;
&lt;p&gt;I was hoping that I could create these files while iterating over the database, as described in the first approach, but that&amp;#8217;s not doable&amp;nbsp;either. &lt;/p&gt;
&lt;p&gt;At the end of the day, the final method is just to write things to disk. Simple beats complicated this time, even when it comes to&amp;nbsp;performance. &lt;/p&gt;</summary><category term="Bulk Data"></category><category term="CourtListener"></category></entry><entry><title>Updating Bulk Data in CourtListener</title><link href="http://michaeljaylissner.com/posts/2014/09/28/updating-bulk-data-in-courtlistener/" rel="alternate"></link><updated>2014-11-06T00:00:00-08:00</updated><author><name>Mike Lissner</name></author><id>tag:michaeljaylissner.com,2014-09-28:posts/2014/09/28/updating-bulk-data-in-courtlistener/</id><summary type="html">
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: I’ve written &lt;a href="http://michaeljaylissner.com/posts/2014/11/06/updating-bulk-data-in-courtlistener-more/"&gt;another post&lt;/a&gt; about how the solution presented here wasn’t fast enough and didn’t work out. You may want to read it as well.&lt;/p&gt;
&lt;p&gt;There’s an increasing demand for bulk data from government systems, and while this will generate big wins for transparency, accountability, and innovation (at the occasional cost of privacy&lt;sup id="fnref:privacy"&gt;&lt;a class="footnote-ref" href="#fn:privacy" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;), it’s important to consider a handful of technical difficulties that &lt;em&gt;can&lt;/em&gt; come along with creating and providing such data. Do &lt;em&gt;not&lt;/em&gt; misread this post as me saying, “bulk data is hard, don’t bother doing it.” Rather, like most of my posts, read this as an in-the-trenches account of issues we’ve encountered and solutions we’ve developed at CourtListener. &lt;/p&gt;
&lt;h2 id="the-past-and-present"&gt;The Past and Present&lt;/h2&gt;
&lt;p&gt;For the past several years we’ve had &lt;a href="https://www.courtlistener.com/api/bulk-info/"&gt;bulk data at CourtListener&lt;/a&gt;, but to be frank, it’s been pretty terrible in a lot of ways. Probably the biggest issue with it was that we created it as a single massive &lt;span class="caps"&gt;XML&lt;/span&gt; file (~&lt;span class="caps"&gt;13GB&lt;/span&gt;, compressed!). That made a lot of sense for our backend processing, but people consuming the bulk data complained that it crashed 32 bit systems&lt;sup id="fnref:sympathy"&gt;&lt;a class="footnote-ref" href="#fn:sympathy" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;, consumed memory endlessly, decompressing it wasn’t possible on Windows&lt;sup id="fnref:sympathy"&gt;&lt;a class="footnote-ref" href="#fn:sympathy" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;, etc. &lt;/p&gt;
&lt;p&gt;On top of these issues for people consuming our bulk data, and even though we set it up to be efficient for our servers, we did a stupid thing when we set it up and made it so our users could generate bulk files whenever they wanted for any day, month, year or jurisdiction. And create bulk files they did. Indeed in the year since we started keeping tabs on this, people made nearly fifty thousand requests for time-based bulk data&lt;sup id="fnref:stats"&gt;&lt;a class="footnote-ref" href="#fn:stats" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;. &lt;/p&gt;
&lt;p&gt;On the backend, the way this worked was that the first time somebody wanted a bulk file, they requested it, we generated it, and then we served it. The second time somebody requested that same file, we just served the one we generated  before, creating a disk-based cache of sorts. This actually worked pretty well but it let people start long-running processes on our server that could degrade the performance of the front end. It wasn’t great, but it was a simple way to serve time- and jurisdiction-based files.&lt;sup id="fnref:file-count"&gt;&lt;a class="footnote-ref" href="#fn:file-count" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt;
&lt;p&gt;As any seasoned developer knows, the next problem with such a system would be cache invalidation. How would we know that a cached bulk file had bad data and how would we delete it if necessary? Turns out this wasn’t so hard, but every time we changed (or deleted) an item in our database we had code that went out to the cache on disk and deleted any bulk files that might contain stale data. Our data doesn’t change often, so for the most part this worked, but it’s the kind of spaghetti code you want to avoid. Touching disk whenever an item is updated? Not so good.  &lt;/p&gt;
&lt;p&gt;And there were bugs. &lt;a href="https://github.com/freelawproject/courtlistener/issues/278"&gt;Weird ones&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Yeah, the old system kind of sucked. The last few days I’ve been busy re-working our bulk data system to make it more reliable, easier to use and just overall, better.&lt;/p&gt;
&lt;h2 id="the-new-system"&gt;The New System&lt;/h2&gt;
&lt;p&gt;Let’s get the bad news taken care of off the bat: The new system no longer allows date-based bulk files. Since these could cause performance issues on the front end, and since &lt;a href="http://lists.freelawproject.org/pipermail/dev/2014-August/000069.html"&gt;nobody opposed the change&lt;/a&gt;, we’ve done away with this feature. It had a good life, may it &lt;span class="caps"&gt;RIP&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The good news is that by getting rid of the date-based bulk files, we’ve been able to eliminate a metric &lt;em&gt;ton&lt;/em&gt; of complexity, &lt;a href="http://theweek.com/article/index/241002/how-the-wrong-definition-of-literally-snuck-into-the-dictionary"&gt;literally&lt;/a&gt;! No longer do we need the disk-cache. No longer do we need to parse URLs and generate bulk data on the fly. No longer is the code a mess of decision trees based on cache state and user requests. Ah, it feels so free at last! &lt;/p&gt;
&lt;p&gt;And it gets even better. On top of this, we were able to resolve &lt;a href="https://github.com/freelawproject/courtlistener/issues/285"&gt;a long-standing feature request&lt;/a&gt; for complete bulk data files by jurisdiction. We were able to make the schema of the bulk files match that of &lt;a href="https://www.courtlistener.com/api/rest-info/"&gt;our &lt;span class="caps"&gt;REST&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt;. We were able to make the bulk file a tar of smaller &lt;span class="caps"&gt;JSON&lt;/span&gt; files, so no more issues unzipping massive files or having 32 bit systems crash. &lt;a href="https://www.youtube.com/watch?v=8vZx7yF_a7M"&gt;We settled all the family business&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Oh, and one more thing: When this goes live, we’ll have bulk files and an &lt;span class="caps"&gt;API&lt;/span&gt; for oral arguments as well — Another CourtListener first. &lt;/p&gt;
&lt;h2 id="jeez-thats-great-whyd-you-wait-so-long"&gt;Jeez, That’s Great, Why’d You Wait So Long?&lt;/h2&gt;
&lt;p&gt;This is a fair question. If it was possible to gain so much so quickly, why didn’t we do it sooner? Well, there are a number of reasons, but at the core, like so many things, it’s because nothing is actually that easy. &lt;/p&gt;
&lt;p&gt;Before we could make these improvements, we needed to: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure the impact on our users &lt;a href="http://lists.freelawproject.org/pipermail/dev/2014-August/000069.html"&gt;wouldn’t be an issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Test the performance of generating more than 350 bulk files at the end of each month&lt;sup id="fnref:dev-aside"&gt;&lt;a class="footnote-ref" href="#fn:dev-aside" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Have &lt;a href="https://www.courtlistener.com/api/rest-info/"&gt;our &lt;span class="caps"&gt;REST&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt; in place so we could use it to generate the bulk files&lt;/li&gt;
&lt;li&gt;Complete &lt;a href="https://github.com/freelawproject/courtlistener/commit/a0e4326d98e9f501ec3e69955d6b5650471686e8#diff-30d04f22c69dda9704be56ec95d9d2c1R68"&gt;performance profiling&lt;/a&gt; to identify &lt;a href="https://github.com/freelawproject/courtlistener/commit/a0e4326d98e9f501ec3e69955d6b5650471686e8#diff-6f850cf75fe2e1d17284e0b701b26b06L47"&gt;hot spots&lt;/a&gt; in the new bulk data &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/freelawproject/courtlistener/commit/52e8eff985fdf75612837cef4d9ef55ad60f29ad#diff-6"&gt;Rewrite the documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And pretty much everything else you can imagine. So, I suppose the answer is: We waited so long because it was hard. &lt;/p&gt;
&lt;p&gt;But being hard is one thing. Another thing is that although a number of organizations have used our bulk data, never has any contributed either energy or resources to fixing the bugs that they reported. Despite the benefits these organizations got from the bulk files, none chose to support the ecosystem from which they benefited. You can imagine how this isn’t particularly motivational for us, but we’re hopeful that with the new and improved system, those using our data will appreciate the quality of the bulk data and consider &lt;a href="https://www.courtlistener.com/donate/"&gt;supporting us&lt;/a&gt; down the road.  &lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;So, without sucking on too many sour grapes, that’s the story behind the upgrades we’re making to the bulk files at CourtListener. At first blush it may seem like a fairly straightforward feature to get in place (and remember, in &lt;em&gt;many&lt;/em&gt; cases bulk data is stupid-easy to do), but we thought it would be interesting to share our experiences so others might compare notes. If you’re a consumer of CourtListener bulk data, we’ll be taking the wraps off of these new features soon, so make sure to watch the &lt;a href="https://free.law"&gt;Free Law Project blog&lt;/a&gt;. If you’re a developer that’s interested in this kind of thing, we’re eager to hear your feedback and any thoughts you might have. &lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:privacy"&gt;
&lt;p&gt;For example, a few days ago some folks got access to &lt;span class="caps"&gt;NYC&lt;/span&gt; taxi information in bulk. In theory it was anonymized using &lt;span class="caps"&gt;MD5&lt;/span&gt; hashing, but because there were a limited number of inputs into the hashing algorithm, all it took to de-anonymize the data was to compute every possible hash (“&lt;a href="https://medium.com/@vijayp/of-taxis-and-rainbows-f6bc289679a1"&gt;computing the 22M hashes took less than 2 minutes&lt;/a&gt;“) and then work backwards from there to the original IDs. While one researcher did that, another one began finding images of celebrities in taxis and figuring out where they went. Privacy is hard. &lt;a class="footnote-backref" href="#fnref:privacy" rev="footnote" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:sympathy"&gt;
&lt;p&gt;I confess I’m not &lt;em&gt;that&lt;/em&gt; sympathetic… &lt;a class="footnote-backref" href="#fnref:sympathy" rev="footnote" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:stats"&gt;
&lt;p&gt;To be exact: 48271 requests, as gathered by our stats module. &lt;a class="footnote-backref" href="#fnref:stats" rev="footnote" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:file-count"&gt;
&lt;p&gt;So far, 17866 files were created this way that haven’t been invalidated, as counted by: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;find&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;maxdepth&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;read&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="n"&gt;dir&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="n"&gt;printf&lt;/span&gt; &lt;span class="s"&gt;"%s:&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt; &lt;span class="s"&gt;"$dir"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt; &lt;span class="s"&gt;"$dir"&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;wc&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a class="footnote-backref" href="#fnref:file-count" rev="footnote" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:dev-aside"&gt;
&lt;p&gt;Commence a fun digression for the developers. As you might expect, aside from compressing bulk files, the bottleneck of generating 350+ bulk files at once is pulling items from the database and converting them to &lt;span class="caps"&gt;JSON&lt;/span&gt;. We tried a few solutions to this problem, but the best we came up with takes advantage of the fact that every item in the database belongs in exactly two bulk files: The all.tar.gz file and the {jurisdiction}.tar.gz file. One way to put the item into both places would be to generate the all.tar.gz file and then generate each of the 350 smaller files. &lt;/p&gt;
&lt;p&gt;That would iterate every item in the database twice, but while making the jurisdiction files you’d have to do a lot of database filtering…something that it’s generally good to avoid. Our solution to this problem is to create a dictionary of open file handles and then to iterate the entire database once. For each item in the database, add it to both the all.tar.gz file and add it to the {jurisdiction}.tar.gz file. Once complete, close all the file handles. For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Get all the courts&lt;/span&gt;
&lt;span class="n"&gt;courts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Court&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;objects&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c"&gt;# Create a dictionary with one key per jurisdiction&lt;/span&gt;
&lt;span class="n"&gt;tar_files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;court&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;courts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;tar_files&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;court&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pk&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tarfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s"&gt;'/tmp/bulk/opinions/&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;.tar.gz'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;court&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pk&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'w:gz'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;compresslevel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Then iterate over everything, adding it to the correct key&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;item_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c"&gt;# Add the json str to the two tarballs&lt;/span&gt;
    &lt;span class="n"&gt;tarinfo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tarfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TarInfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;.json"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pk&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;tar_files&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;docket&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;court_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addfile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;tarinfo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json_str&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;tar_files&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'all'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addfile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;tarinfo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json_str&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In a sense the first part creates a variable for every jurisdiction on the fly and the second part uses that variable as a dumping point for each item as it iterates over them. &lt;/p&gt;
&lt;p&gt;A fine hack. &lt;a class="footnote-backref" href="#fnref:dev-aside" rev="footnote" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="Bulk Data"></category><category term="CourtListener"></category></entry><entry><title>Viewsonic Drivers, Repair and Service Manuals</title><link href="http://michaeljaylissner.com/posts/2011/02/20/viewsonic-repair-manuals/" rel="alternate"></link><updated>2011-02-20T00:33:40-08:00</updated><author><name>Mike Lissner</name></author><id>tag:michaeljaylissner.com,2011-02-20:posts/2011/02/20/viewsonic-repair-manuals/</id><summary type="html">&lt;p&gt;Don&amp;#8217;t ask me how I came upon these, but please enjoy your hacking and&amp;nbsp;repairs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/17PS-2_SERVICE_MANUAL.pdf"&gt;&lt;span class="caps"&gt;17PS&lt;/span&gt;-2_SERVICE_MANUAL.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/a90fplus-1_SM_1a.pdf"&gt;a90fplus-1_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/cd1010-1_ug_eng.pdf"&gt;cd1010-1_ug_eng.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/E90fB-4_SM_Rev.1a.pdf"&gt;E90fB-4_SM_Rev.1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/G90f_SM_1a.pdf"&gt;G90f_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/KU206-306.zip"&gt;&lt;span class="caps"&gt;KU206&lt;/span&gt;-306.zip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/N2635w-1M_SM_Rev.1a_Jul.2007.pdf"&gt;N2635w-1M_SM_Rev.1a_Jul.2007.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/n2750w-1_sm.pdf"&gt;n2750w-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/n2750w-2g_sm.pdf"&gt;n2750w-2g_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/N3235w-1M_SM_1a.pdf"&gt;N3235w-1M_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/n3250w-1g_sm.pdf"&gt;n3250w-1g_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/N3250w-1L_SM_Rev.1b_Aug.2008.pdf"&gt;N3250w-1L_SM_Rev.1b_Aug.2008.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/n3250w-1_sm_1a.pdf"&gt;n3250w-1_sm_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/N3250wb-1G_SM_1a.pdf"&gt;N3250wb-1G_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/N3250w-G_SM_1a.pdf"&gt;N3250w-G_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/N3252w-1M_SM_Rev.1b_Aug.2008.pdf"&gt;N3252w-1M_SM_Rev.1b_Aug.2008.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/n3260w-1_sm.pdf"&gt;n3260w-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/n3752w-1_sm.pdf"&gt;n3752w-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/n4060w-1nt_sm.pdf"&gt;n4060w-1nt_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/n4200w-1_sm.pdf"&gt;n4200w-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/N4251w-1M_SM_Rev.1b_Jul.2007.pdf"&gt;N4251w-1M_SM_Rev.1b_Jul.2007.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/N4261w-M_SM.pdf"&gt;N4261w-M_SM.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/NMP530_2.43.zip"&gt;NMP530_2.43.zip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/NMP530_2.50_firmware_update.zip"&gt;NMP530_2.50_firmware_update.zip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/P90f-1_SM_1a.pdf"&gt;P90f-1_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/pj1158-1_sm.pdf"&gt;pj1158-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/PJ402D-1_SM_1a.pdf"&gt;&lt;span class="caps"&gt;PJ402D&lt;/span&gt;-1_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/PJ402D-2_SM_Rev.1e_Dec.2008.pdf"&gt;&lt;span class="caps"&gt;PJ402D&lt;/span&gt;-2_SM_Rev.1e_Dec.2008.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/PJ501_SM_1a.pdf"&gt;PJ501_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/PJ513D_SM_Rev.1b_May.2009.pdf"&gt;PJ513D_SM_Rev.1b_May.2009.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/PJ650_UG_French.pdf"&gt;PJ650_UG_French.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/PJ875_SM_1a.pdf"&gt;PJ875_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/Pro8100_SM_Rev.1b_Dec.2008.pdf"&gt;Pro8100_SM_Rev.1b_Dec.2008.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/UBRC-100.pdf"&gt;&lt;span class="caps"&gt;UBRC&lt;/span&gt;-100.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/UBRC remote codes.pdf"&gt;&lt;span class="caps"&gt;UBRC&lt;/span&gt; remote&amp;nbsp;codes.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/V1100 Drivers and Utilities on C.zip"&gt;V1100 Drivers and Utilities on&amp;nbsp;C.zip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/va702-1_sm.pdf"&gt;va702-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/va912-4_ug_eng~1a.pdf"&gt;va912-4_ug_eng~1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/ve155-1_SM_1a.pdf"&gt;ve155-1_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vg150m-1_SM_1a.pdf"&gt;vg150m-1_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vg2030m-1_sm.pdf"&gt;vg2030m-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vg800b-1_SM_1a.pdf"&gt;vg800b-1_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vg930m-3_sm.pdf"&gt;vg930m-3_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/VP2030b-1_SM_1a .pdf"&gt;VP2030b-1_SM_1a&amp;nbsp;.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/VP2130b_SM_1a.pdf"&gt;VP2130b_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vp2330wb-1_sm.pdf"&gt;vp2330wb-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vp930-2_sm.pdf"&gt;vp930-2_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/VPW425_SM_1a.pdf"&gt;VPW425_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vt2430-1m_ug_eng.pdf"&gt;vt2430-1m_ug_eng.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/VT2730-1M_SM_1a.pdf"&gt;&lt;span class="caps"&gt;VT2730&lt;/span&gt;-1M_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/VTMS2431_SM.pdf"&gt;VTMS2431_SM.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/VTMS2431_SM Rev. 1a Apr. 2009.pdf"&gt;VTMS2431_SM Rev. 1a Apr.&amp;nbsp;2009.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vx1935wm-3_sm.pdf"&gt;vx1935wm-3_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vx2245wm-1_sm.pdf"&gt;vx2245wm-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/VX2835wm-1_SM.pdf"&gt;VX2835wm-1_SM.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/vx922-1_sm.pdf"&gt;vx922-1_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/VX924-1_SM_1a.pdf"&gt;&lt;span class="caps"&gt;VX924&lt;/span&gt;-1_SM_1a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://michaeljaylissner.com/archive/viewsonic-manuals/WPG150V1501030019.zip"&gt;&lt;span class="caps"&gt;WPG150V1501030019&lt;/span&gt;.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="viewsonic"></category><category term="manuals"></category><category term="bulk data"></category></entry></feed>